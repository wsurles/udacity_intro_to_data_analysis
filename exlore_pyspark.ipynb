{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark Interactive Shell Quickstart \n",
    "\n",
    "Copying [this tutorial](http://spark.apache.org/docs/latest/quick-start.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x10f032cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'# Apache Spark'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_spark = text_file.filter(lambda line: \"Spark\" in line)\n",
    "lines_with_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do some old school `map` and `reduce` on the text file to find the longest line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.map(lambda line: len(line.split())).reduce(lambda a, b: a if(a > b) else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do this is to create function and apply them in the map and reduce instead of lambdas.  \n",
    "This is definitely easier to follow what is happening. \n",
    "\n",
    "We first map the len_line function to each row in the text_file.   \n",
    "I guess the map function naturally handles applying our function to all the lines.\n",
    "\n",
    "Then we apply the max funciton inside the reduce step.   \n",
    "I guess this sorta loops through the elements and takes two at a time to compare. \n",
    "\n",
    "I don't fully understand how the map and reduce functions work yet but the following code successfully counts the length of the longest line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max(a, b):\n",
    "    if a > b:\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "\n",
    "def len_line(line):\n",
    "    return len(line.split())\n",
    "    \n",
    "text_file.map(len_line).reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aw yeah, here is a simple way to write this line by line instead of one long line.  \n",
    "I like this much better for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file \\\n",
    "    .map(lambda line: len(line.split())) \\\n",
    "    .reduce(lambda a, b: a if(a > b) else b)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'all', 1),\n",
       " (u'Requirements', 1),\n",
       " (u'R,', 1),\n",
       " (u'including', 2),\n",
       " (u'computation', 1),\n",
       " (u'standalone,', 1),\n",
       " (u'PySpark.', 1),\n",
       " (u'Scala,', 1),\n",
       " (u'only', 1),\n",
       " (u'rich', 1),\n",
       " (u'Apache', 1),\n",
       " (u'guide,', 1),\n",
       " (u'Mesos)', 1),\n",
       " (u'matches', 1),\n",
       " (u'not', 2),\n",
       " (u'Spark', 13),\n",
       " (u'documentation,', 1),\n",
       " (u'cluster.', 1),\n",
       " (u'compatibility).', 1),\n",
       " (u'GraphX', 1),\n",
       " (u'[project', 1),\n",
       " (u'##', 3),\n",
       " (u'related', 1),\n",
       " (u'see', 1),\n",
       " (u'download', 1),\n",
       " (u'[Apache', 1),\n",
       " (u'odd', 1),\n",
       " (u'best', 1),\n",
       " (u'#', 1),\n",
       " (u'0.10.4),', 1),\n",
       " (u'processing.', 1),\n",
       " (u'(be', 1),\n",
       " (u'please', 1),\n",
       " (u'provides', 1),\n",
       " (u'supports', 2),\n",
       " (u'we', 1),\n",
       " (u'This', 3),\n",
       " (u'packaged', 1),\n",
       " (u'change', 1),\n",
       " (u'programming', 1),\n",
       " (u'experience', 1),\n",
       " (u'Packaging', 1),\n",
       " (u'ensure', 1),\n",
       " (u'experimental', 1),\n",
       " (u'errors.', 1),\n",
       " (u'packaging', 2),\n",
       " (u'tools', 2),\n",
       " (u'use', 1),\n",
       " (u'from', 2),\n",
       " (u'fast', 1),\n",
       " (u'<http://spark.apache.org/>', 1),\n",
       " (u'README', 1),\n",
       " (u'numpy', 1),\n",
       " (u'minor', 1),\n",
       " (u'engine', 1),\n",
       " (u'building', 1),\n",
       " (u'but', 2),\n",
       " (u'with', 2),\n",
       " (u'this', 2),\n",
       " (u'setup', 1),\n",
       " (u'depends', 1),\n",
       " (u'will', 1),\n",
       " (u'and', 8),\n",
       " (u'have', 1),\n",
       " (u'stream', 1),\n",
       " (u'sub-packages', 1),\n",
       " (u'is', 4),\n",
       " (u'higher-level', 1),\n",
       " (u'file', 1),\n",
       " (u'pip', 1),\n",
       " (u'You', 2),\n",
       " (u'requirements', 1),\n",
       " (u'(currently', 1),\n",
       " (u'Python', 4),\n",
       " (u'-', 1),\n",
       " (u'pandas).', 1),\n",
       " (u'machine', 1),\n",
       " (u'you', 4),\n",
       " (u'may', 2),\n",
       " (u'downloads', 1),\n",
       " (u'optimized', 1),\n",
       " (u'processing,', 1),\n",
       " (u'The', 1),\n",
       " (u'data', 1),\n",
       " (u'a', 4),\n",
       " (u'for', 8),\n",
       " (u'builder', 1),\n",
       " (u'the', 9),\n",
       " (u'requires', 1),\n",
       " (u'PySpark', 2),\n",
       " (u'existing', 1),\n",
       " (u'high-level', 1),\n",
       " (u'find', 1),\n",
       " (u'web', 1),\n",
       " (u'graph', 1),\n",
       " (u'source', 1),\n",
       " (u'JARs,', 1),\n",
       " (u'Using', 1),\n",
       " (u'do', 1),\n",
       " (u'YARN,', 1),\n",
       " (u'using', 1),\n",
       " (u'installed', 1),\n",
       " (u'instructions', 1),\n",
       " (u'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " (u'Streaming', 1),\n",
       " (u'contain', 1),\n",
       " (u'set', 1),\n",
       " (u'are', 2),\n",
       " (u'our', 1),\n",
       " (u'Python,', 1),\n",
       " (u'interacting', 1),\n",
       " (u'currently', 1),\n",
       " (u'its', 1),\n",
       " (u'version', 4),\n",
       " (u'core', 1),\n",
       " (u'full', 1),\n",
       " (u'Documentation', 1),\n",
       " (u'[\"Building', 1),\n",
       " (u'standalone', 2),\n",
       " (u'of', 4),\n",
       " (u'APIs', 1),\n",
       " (u'keep', 1),\n",
       " (u'(including', 2),\n",
       " (u'Big', 1),\n",
       " (u'suitable', 1),\n",
       " (u'or', 2),\n",
       " (u'learning,', 1),\n",
       " (u'own', 2),\n",
       " (u'SQL', 2),\n",
       " (u'replace', 1),\n",
       " (u'analysis.', 1),\n",
       " (u'Online', 1),\n",
       " (u'your', 1),\n",
       " (u'additional', 1),\n",
       " (u'to', 4),\n",
       " (u'contains', 1),\n",
       " (u'system', 1),\n",
       " (u'does', 1),\n",
       " (u'basic', 1),\n",
       " (u'version)', 1),\n",
       " (u'page](http://spark.apache.org/downloads.html).', 1),\n",
       " (u'that', 2),\n",
       " (u'DataFrames,', 1),\n",
       " (u'an', 2),\n",
       " (u'must', 1),\n",
       " (u'versions', 1),\n",
       " (u'future', 1),\n",
       " (u'can', 2),\n",
       " (u'Data.', 1),\n",
       " (u'computing', 1),\n",
       " (u'**NOTE:**', 1),\n",
       " (u'it', 1),\n",
       " (u'general', 2),\n",
       " (u'cluster', 3),\n",
       " (u'at', 1),\n",
       " (u'in', 2),\n",
       " (u'their', 1),\n",
       " (u'if', 1),\n",
       " (u'information', 1),\n",
       " (u'Java,', 1),\n",
       " (u'MLlib', 1),\n",
       " (u'also', 1),\n",
       " (u'other', 1),\n",
       " (u'cases.', 1),\n",
       " (u'Py4J', 1),\n",
       " (u'intended', 1),\n",
       " (u'on', 2),\n",
       " (u'page](http://spark.apache.org/documentation.html)', 1),\n",
       " (u'required', 1),\n",
       " (u'It', 2),\n",
       " (u'(although', 1),\n",
       " (u'graphs', 1),\n",
       " (u'At', 1),\n",
       " (u'latest', 1),\n",
       " (u'If', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts = text_file \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "wordCounts.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do this again but sort the fields.  \n",
    "Unfortunately in pyspark sorting is a bit of a hack. : (\n",
    "\n",
    "To sort by the values, and not the key, I have to do  swap with the map function...twice.  \n",
    "Swap, sort, then swap back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Spark', 13),\n",
       " (u'the', 9),\n",
       " (u'and', 8),\n",
       " (u'for', 8),\n",
       " (u'is', 4),\n",
       " (u'Python', 4),\n",
       " (u'you', 4),\n",
       " (u'a', 4),\n",
       " (u'version', 4),\n",
       " (u'of', 4),\n",
       " (u'to', 4),\n",
       " (u'##', 3),\n",
       " (u'This', 3),\n",
       " (u'cluster', 3),\n",
       " (u'including', 2),\n",
       " (u'not', 2),\n",
       " (u'supports', 2),\n",
       " (u'packaging', 2),\n",
       " (u'tools', 2),\n",
       " (u'from', 2),\n",
       " (u'but', 2),\n",
       " (u'with', 2),\n",
       " (u'this', 2),\n",
       " (u'You', 2),\n",
       " (u'may', 2),\n",
       " (u'PySpark', 2),\n",
       " (u'are', 2),\n",
       " (u'standalone', 2),\n",
       " (u'(including', 2),\n",
       " (u'or', 2),\n",
       " (u'own', 2),\n",
       " (u'SQL', 2),\n",
       " (u'that', 2),\n",
       " (u'an', 2),\n",
       " (u'can', 2),\n",
       " (u'general', 2),\n",
       " (u'in', 2),\n",
       " (u'on', 2),\n",
       " (u'It', 2),\n",
       " (u'all', 1),\n",
       " (u'Requirements', 1),\n",
       " (u'R,', 1),\n",
       " (u'computation', 1),\n",
       " (u'standalone,', 1),\n",
       " (u'PySpark.', 1),\n",
       " (u'Scala,', 1),\n",
       " (u'only', 1),\n",
       " (u'rich', 1),\n",
       " (u'Apache', 1),\n",
       " (u'guide,', 1),\n",
       " (u'Mesos)', 1),\n",
       " (u'matches', 1),\n",
       " (u'documentation,', 1),\n",
       " (u'cluster.', 1),\n",
       " (u'compatibility).', 1),\n",
       " (u'GraphX', 1),\n",
       " (u'[project', 1),\n",
       " (u'related', 1),\n",
       " (u'see', 1),\n",
       " (u'download', 1),\n",
       " (u'[Apache', 1),\n",
       " (u'odd', 1),\n",
       " (u'best', 1),\n",
       " (u'#', 1),\n",
       " (u'0.10.4),', 1),\n",
       " (u'processing.', 1),\n",
       " (u'(be', 1),\n",
       " (u'please', 1),\n",
       " (u'provides', 1),\n",
       " (u'we', 1),\n",
       " (u'packaged', 1),\n",
       " (u'change', 1),\n",
       " (u'programming', 1),\n",
       " (u'experience', 1),\n",
       " (u'Packaging', 1),\n",
       " (u'ensure', 1),\n",
       " (u'experimental', 1),\n",
       " (u'errors.', 1),\n",
       " (u'use', 1),\n",
       " (u'fast', 1),\n",
       " (u'<http://spark.apache.org/>', 1),\n",
       " (u'README', 1),\n",
       " (u'numpy', 1),\n",
       " (u'minor', 1),\n",
       " (u'engine', 1),\n",
       " (u'building', 1),\n",
       " (u'setup', 1),\n",
       " (u'depends', 1),\n",
       " (u'will', 1),\n",
       " (u'have', 1),\n",
       " (u'stream', 1),\n",
       " (u'sub-packages', 1),\n",
       " (u'higher-level', 1),\n",
       " (u'file', 1),\n",
       " (u'pip', 1),\n",
       " (u'requirements', 1),\n",
       " (u'(currently', 1),\n",
       " (u'-', 1),\n",
       " (u'pandas).', 1),\n",
       " (u'machine', 1),\n",
       " (u'downloads', 1),\n",
       " (u'optimized', 1),\n",
       " (u'processing,', 1),\n",
       " (u'The', 1),\n",
       " (u'data', 1),\n",
       " (u'builder', 1),\n",
       " (u'requires', 1),\n",
       " (u'existing', 1),\n",
       " (u'high-level', 1),\n",
       " (u'find', 1),\n",
       " (u'web', 1),\n",
       " (u'graph', 1),\n",
       " (u'source', 1),\n",
       " (u'JARs,', 1),\n",
       " (u'Using', 1),\n",
       " (u'do', 1),\n",
       " (u'YARN,', 1),\n",
       " (u'using', 1),\n",
       " (u'installed', 1),\n",
       " (u'instructions', 1),\n",
       " (u'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " (u'Streaming', 1),\n",
       " (u'contain', 1),\n",
       " (u'set', 1),\n",
       " (u'our', 1),\n",
       " (u'Python,', 1),\n",
       " (u'interacting', 1),\n",
       " (u'currently', 1),\n",
       " (u'its', 1),\n",
       " (u'core', 1),\n",
       " (u'full', 1),\n",
       " (u'Documentation', 1),\n",
       " (u'[\"Building', 1),\n",
       " (u'APIs', 1),\n",
       " (u'keep', 1),\n",
       " (u'Big', 1),\n",
       " (u'suitable', 1),\n",
       " (u'learning,', 1),\n",
       " (u'replace', 1),\n",
       " (u'analysis.', 1),\n",
       " (u'Online', 1),\n",
       " (u'your', 1),\n",
       " (u'additional', 1),\n",
       " (u'contains', 1),\n",
       " (u'system', 1),\n",
       " (u'does', 1),\n",
       " (u'basic', 1),\n",
       " (u'version)', 1),\n",
       " (u'page](http://spark.apache.org/downloads.html).', 1),\n",
       " (u'DataFrames,', 1),\n",
       " (u'must', 1),\n",
       " (u'versions', 1),\n",
       " (u'future', 1),\n",
       " (u'Data.', 1),\n",
       " (u'computing', 1),\n",
       " (u'**NOTE:**', 1),\n",
       " (u'it', 1),\n",
       " (u'at', 1),\n",
       " (u'their', 1),\n",
       " (u'if', 1),\n",
       " (u'information', 1),\n",
       " (u'Java,', 1),\n",
       " (u'MLlib', 1),\n",
       " (u'also', 1),\n",
       " (u'other', 1),\n",
       " (u'cases.', 1),\n",
       " (u'Py4J', 1),\n",
       " (u'intended', 1),\n",
       " (u'page](http://spark.apache.org/documentation.html)', 1),\n",
       " (u'required', 1),\n",
       " (u'(although', 1),\n",
       " (u'graphs', 1),\n",
       " (u'At', 1),\n",
       " (u'latest', 1),\n",
       " (u'If', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts = text_file \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .map(lambda (a, b): (b, a)) \\\n",
    "    .sortByKey(False, 1) \\\n",
    "    .map(lambda (a, b): (b, a))\n",
    "\n",
    "wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, thats probably good for this initial explore.  \n",
    "I'm not sure how I will use spark the most in the future.  \n",
    "I may use spark SQL or sparkR much more.   \n",
    "I can't really seeing myself relying on map and reduce functions in pyspark.  \n",
    "There must be a better way!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
